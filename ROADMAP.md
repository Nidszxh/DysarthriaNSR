# Implementation Status & Architecture (February 2026)

## Current Status ‚úÖ OPERATIONAL

The complete end-to-end pipeline is **fully implemented** and **production-ready**:
- ‚úÖ Data download and extraction
- ‚úÖ Neuro-symbolic manifest generation
- ‚úÖ PyTorch Dataset with automatic feature extraction
- ‚úÖ Multi-task learning (CTC + CE + articulatory heads)
- ‚úÖ PyTorch Lightning training with MLflow integration
- ‚úÖ Comprehensive evaluation (PER/WER with bootstrap CI)
- ‚úÖ Beam search decoding

**Latest Results: baseline_v1 (January 2026)**
| Metric | Value |
|--------|-------|
| **Test PER** | 0.567 ¬± 0.365 (3,553 samples) |
| Dysarthric PER | 0.541 (12 speakers) |
| Control PER | 0.575 (3 speakers) |
| Model size | 94.8M params (416K trainable) |
| Best epoch | 17/50 (val/per = 0.503) |
| Training time | ~12 hours (RTX 4060) |

**Known Issue**: High insertion rate (21,290) indicates CTC blank suppression. See README.md for mitigation strategies.

## System Architecture

```
download.py (HF TORGO ‚Üí audio files)
  ‚Üì
data/processed/raw_extraction_map.csv
  ‚Üì
manifest.py (phonemes + articulatory features)
  ‚Üì
data/processed/torgo_neuro_symbolic_manifest.csv
  ‚Üì
dataloader.py (TorgoNeuroSymbolicDataset + NeuroSymbolicCollator)
  ‚Üì
train.py (DysarthriaASRLightning + NeuroSymbolicASR)
  ‚Üì (automatic after training)
evaluate.py (metrics + diagnostics)
  ‚Üì
results/{run_name}/ (JSON results + confusion matrix PNG)

## Data Pipeline (‚úÖ Fully Implemented)

### 1. Download (`src/data/download.py`)
- **Source**: HuggingFace `abnerh/TORGO-database` (26GB)
- **Output**: Audio files under `data/raw/audio/` with naming scheme: `{speaker}_{hash}_{original_name}.wav`
- **Normalization**: Resamples to 16 kHz (standardized for HuBERT)
- **Logging**: `data/processed/raw_extraction_map.csv` maps speaker/filename to hash ID

**Key implementation details**:
- Thread-safe parallel extraction using `ThreadPoolExecutor`
- HF `Audio(decode=False)` for path-only metadata access (avoids memory overhead)
- Error handling: Logs failed samples; continues gracefully

### 2. Manifest Generation (`src/data/manifest.py`)
**Inputs**: Local audio + HF metadata  
**Output**: `data/processed/torgo_neuro_symbolic_manifest.csv` (16,552 samples, ~13.7 hours)

**Processing pipeline**:
1. **Phoneme extraction** (g2p_en):
   - Convert text transcripts ‚Üí ARPABET phonemes (44 symbols)
   - Stress-normalize via `normalize_phoneme()` (e.g., AH0 ‚Üí AH)
   - Validate min 2 phonemes per utterance; warn on high phoneme density (>20/sec)

2. **Articulatory feature mapping**:
   - For each phoneme, assign (manner, place, voicing) from PHONEME_DETAILS dict
   - Example: B ‚Üí (stop, bilabial, voiced)
   - Enables symbolic constraint matrix computation

3. **Audio metrics**:
   - RMS energy (loudness normalization indicator)
   - Duration + phoneme rate (slow dysarthric speech detection)
   - Peak amplitude (dysarthric breath support variability)

4. **Validation**:
   - Remove empty transcripts
   - Check audio file existence
   - Deduplicate by speaker+path

**Dataframe columns**:
```
['path', 'speaker', 'status', 'phonemes', 'manner', 'place', 'voicing', 
 'rms_energy', 'duration', 'peak_amplitude', 'label', 'sample_id']
```

### 3. Dataset & Dataloader (`src/data/dataloader.py`)

#### Class: `TorgoNeuroSymbolicDataset(Dataset)`
**Inputs**: Manifest CSV path + processor ID (default: `facebook/hubert-base-ls960`)

**Initialization**:
1. Load manifest; validate & clean (remove empty phonemes)
2. Load HuBERT feature extractor from HF
3. Build vocabularies:
   - **Phoneme vocab**: `<BLANK>`(0), `<PAD>`(1), `<UNK>`(2), then 44 ARPABET phonemes (3‚Äì46)
   - **Articulatory vocabs**: manner/place/voicing (e.g., {stop, fricative, ...} ‚Üí indices)
4. Pre-compute inverse-frequency weights for class balancing:
   - `phoneme_weights[i] = total_phonemes / count[i]` (normalized)
   - `articulatory_weights` for manner/place/voicing
5. Build speaker-to-indices mappings for stratified splitting

**`__getitem__` pipeline**:
1. **Audio loading** (`_load_audio`):
   - Load WAV from path at 16 kHz
   - Peak-normalize: `audio / max(abs(audio))`
   - Truncate to `max_audio_length` (6.0s default)
   
2. **Feature extraction**:
   - Pass audio through HuBERT processor
   - Output shape: `[1, num_frames]` at 50 Hz (16000 Hz √∑ 320 stride)
   
3. **Label encoding**:
   - Convert phoneme string ‚Üí integer sequence
   - Unknown phonemes ‚Üí `<UNK>` (ID 2)
   - Pad/truncate to match logits time dimension using `-100` sentinel

4. **Articulatory labels**:
   - Map each phoneme ‚Üí manner/place/voicing IDs
   - Frame-level labels aligned with feature temporal dimension

**Sample structure**:
```python
{
    'input_values': torch.Tensor([num_frames]),  # HuBERT features
    'labels': torch.Tensor([max_seq_len]),  # Phoneme IDs (-100 for padding)
    'manner_labels': torch.Tensor([...]),  # Manner class per phoneme
    'place_labels': torch.Tensor([...]),   # Place class per phoneme
    'voicing_labels': torch.Tensor([...]), # Voicing class per phoneme
    'speaker': str, 
    'status': int (0=control, 1=dysarthric),
    'sample_id': str
}
```

#### Class: `NeuroSymbolicCollator`
**Purpose**: Batch processing with CTC-aware padding

**Key operations**:
1. **Pad input features**: `torch.nn.utils.rnn.pad_sequence()`
   - All samples padded to max length in batch
   - `attention_mask` computed: 1 for valid frames, 0 for padding

2. **Compute input_lengths**: `len(feature) // 320` (CTC stride for HuBERT)
   - Used by CTCLoss to ignore padding frames

3. **Pad labels with -100**: `torch.nn.functional.pad()` with fill_value=-100
   - CTC loss ignores -100 labels automatically
   - Frame-level CE loss also ignores -100

4. **Class weight assignment**:
   - For each sample, assign pre-computed `phoneme_weights` to labels
   - For each articulatory class, assign corresponding weights
   - Used in CE loss computation

5. **Encode speaker severity**:
   - status (0/1) ‚Üí severity (0.0‚Äì5.0) for adaptive beta in model forward

**Output batch structure**:
```python
{
    'input_values': [batch, max_frames],
    'attention_mask': [batch, max_frames],
    'labels': [batch, max_seq_len],  # Phoneme labels with -100 padding
    'manner_labels': [batch, ...],
    'place_labels': [batch, ...],
    'voicing_labels': [batch, ...],
    'input_lengths': [batch],  # For CTC
    'label_lengths': [batch],  # For CTC
    'speaker': List[str],
    'status': [batch],  # 0/1
    'phoneme_weights': [batch, max_seq_len],  # Per-label class weights
    'articulatory_weights': {...}  # dict of per-class weights
}

## Model Architecture (‚úÖ Fully Implemented)

### Class: `NeuroSymbolicASR(nn.Module)`
Located in `src/models/model.py` (764 lines)

**Components**:

1. **HuBERT Encoder** (`transformers.HuBertModel`)
   - Pre-trained base model: `facebook/hubert-base-ls960`
   - 12 transformer layers, 768-D hidden dim, 95M params
   - **Freezing strategy**: First 10 layers frozen (ModelConfig.freeze_encoder_layers)
     - Rationale: Lower layers learn generic acoustic features
     - Cost: ~8.2GB VRAM ‚Üí ~5.5GB with freezing
   - **Gradient checkpointing**: Enabled for remaining layers to save activation memory
   - **Output**: [batch, frames, 768] at 50 Hz frame rate

2. **Phoneme Classifier Head**
   - Architecture: 768 ‚Üí 512 ‚Üí 44 (ARPABET phonemes)
   - Dropout: 0.1 for regularization
   - Activation: ReLU between layers
   - **Logits output**: [batch, frames, 44]

3. **Articulatory Auxiliary Heads** (optional)
   - **Manner classification**: 768 ‚Üí 512 ‚Üí {stop, fricative, nasal, liquid, glide, vowel, ...} (~10 classes)
   - **Place classification**: 768 ‚Üí 512 ‚Üí {bilabial, alveolar, velar, ...} (~10 classes)
   - **Voicing classification**: 768 ‚Üí 512 ‚Üí {voiced, voiceless} (2 classes)
   - **Purpose**: Implicit regularization; helps model learn articulatory structure
   - **Training loss weight**: Œª_art = 0.1 (vs. Œª_ctc = 0.8, Œª_ce = 0.2)

4. **Symbolic Constraint Layer** (`SymbolicConstraintLayer` + `ConstraintAggregation`)
   - **Input**: Logits [batch, frames, 44]
   - **Constraint matrix C**: 44√ó44 similarity matrix
   
   **C matrix construction**:
   ```python
   C[i][i] = 1.0  # Identity (correct phoneme reinforcement)
   
   # Dysarthric substitution rules (hard-coded, from SymbolicConfig)
   for (source_phn, target_phn), prob in substitution_rules.items():
       C[source_id][target_id] = prob
       # Example: B‚ÜíP (devoicing) = 0.85
   
   # Articulatory similarity (if use_articulatory_distance):
   for ph1, ph2 in all_phoneme_pairs:
       if not in substitution_rules:
           C[ph1_id][ph2_id] = ArticulatoryDistance(ph1, ph2, weights)
   ```
   
   **Articulatory distance function**:
   ```
   distance = sqrt(
       w_manner * (f1.manner ‚â† f2.manner) +
       w_place  * (f1.place ‚â† f2.place) +
       w_voice  * (f1.voice ‚â† f2.voice)
   )
   similarity = exp(-decay_factor * distance)
   ```
   Default weights: manner=0.4, place=0.35, voicing=0.25

5. **Constraint Aggregation** (`ConstraintAggregation(nn.Module)`)
   - **Learnable beta parameter**: Controls neural vs. symbolic blending
   - **Formula**: 
     ```
     P_constrained = P_neural @ C  # Apply constraint matrix
     P_final = Œ≤ ¬∑ P_constrained + (1-Œ≤) ¬∑ P_neural
     ```
   - **Initialization**: Œ≤ = 0.05 (favor neural, constraints as regularizer)
   - **Learning**: Œ≤ is trainable parameter, updated via backprop
   - **Clamping**: Œ≤ clamped to [0.0, 0.8] to prevent over-reliance on constraints
   - **Severity adaptation** (forward pass):
     ```
     severity = status * 5.0  # 0 (control) or 5.0 (dysarthric)
     Œ≤_speaker_adaptive = Œ≤ + (severity * severity_beta_slope)
     ```
     Makes constraints stronger for dysarthric speakers

### Key Architectural Decisions

| Decision | Rationale | Trade-off |
|----------|-----------|-----------|
| Freeze first 10/12 layers | VRAM: 5.5GB vs. 8.2GB unfrozen | Small loss in model capacity (~1-2% PER) |
| CTC + CE multi-task | CTC handles variable-length, CE provides frame supervision | +0.3-0.5GB VRAM for auxiliary heads |
| Articulatory auxiliary heads | Implicit regularization for phonetic structure | Model learns to predict manner/place without explicit gold labels |
| Learnable Œ≤ (not fixed) | Allows model to discover optimal neural-symbolic balance | Can overfit to training data-specific balance |
| Severity-adaptive Œ≤ | Makes constraints stronger for dysarthric speakers | Requires reliable severity labels (status=0/1) |
| Beam search width=10 | Balance precision and inference speed | Wider beams (16/20) improve PER ~0.5-1% at 2-3√ó latency |

## Training Pipeline (‚úÖ Fully Implemented)

### Framework: PyTorch Lightning (`train.py`, 980 lines)

**Class**: `DysarthriaASRLightning(pl.LightningModule)`

**Key methods**:
- `forward()`: Model forward pass with speaker severity scaling
- `training_step()`: Compute multi-task loss (CTC + CE + articulatory CE)
- `validation_step()`: Eval mode, compute PER on validation set
- `on_train_epoch_end()`: Optionally unfreeze encoder after warmup_epochs

**Loss computation** (`compute_loss()`):
```python
loss_ctc = CTCLoss(logits, labels, input_lengths, label_lengths)
loss_ce = CrossEntropyLoss(logits_frame, labels, class_weights=phoneme_weights)
loss_manner = CrossEntropyLoss(manner_logits, manner_labels, ...)
loss_place = CrossEntropyLoss(place_logits, place_labels, ...)
loss_voicing = CrossEntropyLoss(voicing_logits, voicing_labels, ...)

total_loss = (
    lambda_ctc * loss_ctc +
    lambda_ce * loss_ce +
    lambda_articulatory * (loss_manner + loss_place + loss_voicing) / 3
)
```

**Multi-task weights** (defaultsin TrainingConfig):
- Œª_ctc = 0.8 (primary alignment objective)
- Œª_ce = 0.2 (frame-level auxiliary supervision)
- Œª_articulatory = 0.1 (articulatory structure regularization)

### Hyperparameter Configuration (Source: `src/utils/config.py`)

**ModelConfig**:
```python
hubert_model_id: str = "facebook/hubert-base-ls960"
freeze_feature_extractor: bool = True
freeze_encoder_layers: List[int] = [0, 1, 2, ..., 9]  # First 10/12 frozen
hidden_dim: int = 512
num_phonemes: int = 44  # ARPABET
constraint_weight_init: float = 0.05
constraint_learnable: bool = True
```

**TrainingConfig** (RTX 4060 optimized):
```python
precision: str = "bf16-mixed"  # BF16 for Ada cards (stable FP32 performance)
batch_size: int = 4  # EffectiveBatch = 4 * 8 = 32
gradient_accumulation_steps: int = 8
learning_rate: float = 3e-5
optimizer: str = "AdamW"
lr_scheduler: str = "onecycle"  # OneCycleLR for warm start + annealing
warmup_steps: int = 250
warmup_ratio: float = 0.05
max_epochs: int = 30
encoder_warmup_epochs: int = 3  # Unfreeze after 3 epochs
val_check_interval: float = 0.5  # Validate twice per epoch
lambda_ctc: float = 0.8
lambda_ce: float = 0.2
lambda_articulatory: float = 0.1
monitor_metric: str = "val/per"
early_stopping_patience: int = 8
save_top_k: int = 2  # Keep best 2 checkpoints
blank_priority_weight: float = 1.5  # Blank class weight boost (for insertion mitigation)
```

**DataConfig**:
```python
max_audio_length: float = 6.0  # seconds (covers ~99% of TORGO)
sampling_rate: int = 16000
train_split: float = 0.7
val_split: float = 0.15
test_split: float = 0.15
split_strategy: str = "speaker_stratified"  # No speaker overlap between splits
```

**ExperimentConfig**:
```python
experiment_name: str = "DysarthriaNSR"
run_name: str = "default_run"  # Override via CLI --run-name
tracking_uri: str = "file:{ProjectPaths().mlruns_dir}"  # MLflow tracking
log_every_n_steps: int = 20
save_predictions: bool = True
save_confusion_matrix: bool = True
```

**SymbolicConfig**:
```python
# 20 dysarthric substitution rules (evidence-based)
substitution_rules: Dict[Tuple[str,str], float] = {
    ('B', 'P'): 0.85,    # Devoicing: voiced stop ‚Üí voiceless
    ('D', 'T'): 0.82,
    ('G', 'K'): 0.80,
    ('V', 'F'): 0.75,
    ('Z', 'S'): 0.78,
    ('R', 'W'): 0.70,    # Liquid gliding
    ('L', 'W'): 0.60,
    # ... (see config.py for full list)
}
manner_weight: float = 0.4
place_weight: float = 0.35
voice_weight: float = 0.25
distance_decay_factor: float = 3.0
```

### Callbacks & Monitoring

**Callbacks**:
- **EarlyStopping**: Monitor val/per, patience=8, stops if no improvement
- **ModelCheckpoint**: Save top 2 models by val/per; periodically save last.ckpt
- **LearningRateMonitor**: Log LR schedule for debugging convergence

**MLflow Integration**:
- **Logged parameters**: Flattened config (MODEL/*, TRAINING/*, DATA/*, etc.)
- **Logged metrics**: train/loss, train/per, val/loss, val/per, train/beta (constraint weight)
- **Logged artifacts**: 
  - Confusion matrix (PNG)
  - Evaluation results (JSON)
  - Best checkpoint (ONNX export optional)

### Data Distribution & Balancing

**Speaker split** (speaker-stratified, no overlap):
- **Train**: 10 speakers (dysarthric + control mixed)
- **Validation**: 2 speakers
- **Test**: 3 speakers
- **Rationale**: Small dataset (15 speakers total); speaker-blind evaluation prevents data leakage

**Class balancing**:
- **Phoneme class weights**: Inverse frequency (rare phonemes weighted higher)
- **WeightedRandomSampler**: Sample speakers with equal probability (some speakers have more samples)
- **Blank weight boost**: `blank_priority_weight = 1.5` in CE loss (attempt to mitigate insertion bias)

### Training Dynamics

**Expected convergence** (baseline_v1):
- **Epoch 1-3**: Encoder frozen; fast gradient flow through unfrozen layers; steep loss descent
- **Epoch 4**: Encoder unfreezes (after 3 warmup epochs); temporary loss plateau; gradients now updated through 12 layers
- **Epoch 5-17**: Steady improvement; val/per drops steadily
- **Epoch 18-30**: Slower improvement; early stopping activates around epoch 25-30 (patience=8)

**Insertion bias observation**:
- Model learns to prefer non-blank emissions early (high insertion count)
- Blank class weight (Œª_blank = 1.5) may be insufficient
- Future: Test Œª_blank = 2.0‚Äì3.0 or KL regularization on blank posterior

## Evaluation Module (‚úÖ Fully Implemented)

### Automatic Evaluation Pipeline
After training completes, `train.py` calls `evaluate_model()` which:

1. **Loads best checkpoint** from ModelCheckpoint callback
2. **Decodes test set** using beam search (width=10)
3. **Computes metrics**: PER, WER, confusion matrices
4. **Generates visualizations**: Confusion matrix heatmap
5. **Saves results** to `results/{run_name}/evaluation_results.json`

### Metrics Computed (Module: `evaluate.py`, 956 lines)

**Phoneme Error Rate (PER)**:
```
PER = (S + D + I) / N

Where:
  S = Substitutions (predicted ‚â† actual, same position)
  D = Deletions (phoneme predicted as blank)
  I = Insertions (extra phonemes predicted)
  N = Total actual phonemes
```

**Confidence intervals**: Bootstrap resampling with 1000 iterations; reports 95% CI

**Word Error Rate (WER)** (optional, requires lexicon):
```
WER = (Sub_w + Del_w + Ins_w) / N_words
```
Computed via phoneme‚Üíword lexicon lookup (CMU pronouncing dictionary)

**Per-speaker breakdown**:
```
results_by_speaker = {
    'speaker_001': {'per': 0.45, 'wer': 0.52, 'num_samples': 450},
    'speaker_002': {'per': 0.58, 'wer': 0.64, 'num_samples': 380},
    ...
}
```

**Per-phoneme metrics**:
```
per_phoneme = {
    'IY': {'correct': 450, 'substitutions': 20, 'deletions': 2, 'insertions': 15, 'per': 0.08},
    'AE': {'correct': 380, 'substitutions': 45, 'deletions': 5, 'insertions': 30, 'per': 0.22},
    ...
}
```

**Confusion matrix**:
- Shape: 44√ó44 (ARPABET phonemes)
- C[i,j] = count of i predicted as j
- Diagonal = correct predictions
- Off-diagonal = errors
- Saved as PNG heatmap for visual inspection

### Beam Search Decoder (Class: `BeamSearchDecoder`)

**Algorithm**: CTC prefix beam search with pruning

**Pseudocode**:
```python
def beam_search(log_probs, id_to_phn, beam_width=10):
    # log_probs: [time, num_classes] matrix of log-softmax outputs
    # id_to_phn: dict mapping class_id ‚Üí phoneme string
    
    # Initialize: empty hypothesis
    beams = [
        (prefix_sequence='', prefix_score=0.0)
    ]
    
    for t in range(len(log_probs)):
        candidates = []
        for prefix, prefix_score in beams:
            for class_id in range(num_classes):
                log_prob = log_probs[t, class_id]
                
                # Prediction: beam search extends with this token
                if class_id == BLANK:
                    # Blank: prefix unchanged
                    new_prefix = prefix
                else:
                    phoneme = id_to_phn[class_id]
                    if prefix.endswith(phoneme):
                        # Skip repetition (CTC merge)
                        continue
                    else:
                        new_prefix = prefix + phoneme
                
                new_score = prefix_score + log_prob
                candidates.append((new_prefix, new_score))
        
        # Prune to top-K by score
        beams = sorted(candidates, key=lambda x: x[1], reverse=True)[:beam_width]
    
    return beams[0][0]  # Return best sequence
```

**Rationale**: More accurate than greedy decoding (which always picks max); accounts for future context via beam width

### Output Structure (`results/{run_name}/evaluation_results.json`)

```json
{
  "overall": {
    "per": 0.567,
    "per_ci_lower": 0.502,
    "per_ci_upper": 0.632,
    "wer": 0.612,
    "num_test_samples": 3553,
    "num_correct": 8853,
    "num_substitutions": 2833,
    "num_deletions": 376,
    "num_insertions": 21290
  },
  "by_speaker": { ... },
  "by_phoneme": { ... },
  "confusion_matrix": [
    [445, 12, 5, ...],  # Phoneme 0 predictions
    [8, 421, 9, ...],  # Phoneme 1 predictions
    ...
  ]
}
```

**CTC Loss Details:**
- Allows variable-length alignment between input frames and phoneme sequence
- Handles repeated phonemes and blank insertions
- Beam search decoding with width=10 during inference

**Focal Loss Formulation:**
```
FL(p_t) = -Œ±_t(1 - p_t)^Œ≥ log(p_t)

Where:
- p_t = predicted probability of true class
- Œ±_t = class weight (inversely proportional to frequency)
- Œ≥ = focusing parameter (Œ≥=2)
```

## Known Issues & Mitigation Strategies

### Issue 1: High CTC Insertion Rate ‚ö†Ô∏è (High Priority)

**Symptom**: Baseline test set shows 21,290 insertions vs. 376 deletions (56√ó ratio imbalance)
- Suggests model over-predicts non-blank phonemes
- Blank frames are under-weighted in loss or backprop

**Root cause hypothesis**:
1. CTC loss doesn't distinguish insertions from substitutions; only cares about alignment cost
2. Frame-level CE loss may not adequately penalize non-blank emissions on silence frames
3. `blank_priority_weight = 1.5` insufficient to force model to predict blanks

**Diagnostic steps**:
```python
# 1. Analyze blank posterior statistics
blank_probs = model_outputs[:, :, BLANK_ID]  # Extract blank probabilities
histogram(blank_probs, bins=50)  # Distribution analysis
print(f"Mean blank prob: {blank_probs.mean():.3f}")
print(f"Median blank prob: {np.median(blank_probs):.3f}")

# 2. Compare dysarthric vs. control
blank_dysarthric = blank_probs[status==1].mean()
blank_control = blank_probs[status==0].mean()
print(f"Blank prob (dysarthric): {blank_dysarthric:.3f}")
print(f"Blank prob (control): {blank_control:.3f}")

# 3. Compute per-frame entropy
entropy = -np.sum(probs * np.log(probs), axis=-1)
print(f"Mean entropy: {entropy.mean():.3f}")  # High entropy = uncertain; low = overconfident
```

**Mitigation options** (in order of effort):

| Strategy | Configuration change | Expected impact |
|----------|----------------------|-----------------|
| Increase blank weight | `blank_priority_weight: 2.0‚Äì3.0` | Directly weighs blank class higher; may suppress non-blanks too much |
| Blank prior KL | Add `KL(P_blank_mean, target=0.3)` regularizer | Soft constraint; allows model some flexibility |
| Insertion-aware decoder | Beam search with insertion cost: `-log(P[i])` | Post-hoc correction; doesn't fix root cause |
| Length penalty | Decoding: reward shorter sequences | Greedy approximation; may hurt other metrics |
| Lower CE weight | `lambda_ce: 0.2 ‚Üí 0.1` | May reduce gradient signal; risky |

**Recommended first experiment**:
```python
# Modify TrainingConfig
blank_priority_weight: 2.5  # Currently 1.5
monitor_metric: val/insertions  # Monitor insertions instead of PER
```

### Issue 2: Dysarthric vs. Control PER Inversion ‚ö†Ô∏è (Medium Priority)

**Observation**: Dysarthric PER (0.541) < Control PER (0.575)
- Counter-intuitive: dysarthric speech should be harder
- Likely confounded by speaker effects (test set only 3 speakers)

**Root cause hypothesis**:
1. Test set may have different speaker-severity distribution than general population
2. Control speakers in test set have more phonetic variety (higher error opportunity)
3. Model overfitted to training dysarthric features

**Diagnostic steps**:
```python
# 1. Stratify by phoneme count
per_by_length = {}
for bins in [(0, 5), (6, 10), (11, 20), (21, 50)]:
    mask = (phoneme_counts >= bins[0]) & (phoneme_counts < bins[1])
    per_by_length[f"{bins[0]}-{bins[1]}"] = per[mask].mean()

# 2. Per-speaker variance
for speaker in speakers:
    mask = test_speakers == speaker
    per_speaker = per[mask].mean()
    dysarthria_status = manifest[manifest.speaker == speaker]['status'].iloc[0]
    print(f"{speaker}: PER={per_speaker:.3f}, status={dysarthria_status}")

# 3. Statistical testing
from scipy.stats import ttest_ind
t_stat, p_val = ttest_ind(per[test_status==1], per[test_status==0])
print(f"t-test: t={t_stat:.3f}, p={p_val:.3f}")
```

**Next steps**:
- Collect more test speakers (dysarthric-only cohort) for unconfounded evaluation
- Publish results separately for dysarthric & control subgroups
- Consider severity-stratified splits (mild vs. moderate vs. severe dysarthria)

### Issue 3: Constraint Weight (Œ≤) Initialization ‚ö†Ô∏è (Low Priority)

**Observation**: Œ≤ converges near 0.5 (balanced neural-symbolic)
- Not clear if this is optimal or just initialization bias

**Investigation**:
```python
# Ablation: Fixed Œ≤ values
variants = [0.0, 0.3, 0.5, 0.7, 0.9, 1.0]
for beta in variants:
    train(constraint_weight_init=beta, constraint_learnable=False)
    # Compare test PER
```

**Expected results**:
- Œ≤=0.0 (symbolic-only): Likely poor (rigid rules can't adapt)
- Œ≤=1.0 (neural-only): Baseline without constraints
- Œ≤=0.5 (balanced): Current learning outcome
- Best Œ≤ likely 0.3‚Äì0.7 depending on other hyperparams

---

## Future Enhancements (Planned, Not Implemented)

### Short-term (1‚Äì2 months)

#### 1. Insertion bias mitigation
- **Task**: Reduce 21K insertions to <10K while maintaining PER
- **Approach**: Blank weight scaling + KL regularizer
- **Success metric**: Test PER ‚â§0.52 with insertions <15K

#### 2. Length-stratified analysis  
- **Task**: Understand dysarthric vs. control gap
- **Approach**: Publish per-length breakdowns; speaker-level CI
- **Output**: Diagnostic report (2-3 pages)

#### 3. Symbolic rule ablation
- **Task**: Quantify neural vs. symbolic contribution
- **Approach**: Neural-only (Œ≤=1.0), symbolic-only (Œ≤=0.0), varying Œ≤
- **Output**: Ablation table in paper

### Medium-term (2‚Äì3 months)

#### 4. Multi-speaker adaptation
- **Task**: Reduce speaker variance in test performance
- **Approach**: Speaker embedding + speaker-specific Œ≤
- **Architecture change**: Add speaker_embedding(speaker_id) ‚Üí scale Œ≤

#### 5. Real-time inference
- **Task**: Enable deployment for speech therapy applications
- **Approach**: ONNX export; streaming CTC decoder (instead of batch)
- **Success metric**: Latency <100ms for real-time feedback

#### 6. Expanded symbol rules  
- **Task**: Auto-extract dysarthric substitution patterns from confusion matrix
- **Approach**: Data-driven rule extraction (threshold-based)
- **Output**: Learned substitution matrix instead of hard-coded

### Long-term (3‚Äì6 months)

#### 7. Domain adaptation to other dysarthria datasets
- **Task**: Test on dysarthric speech outside TORGO (e.g., UASPEECH, DysarthricSpeech dataset)
- **Challenge**: Different speaker demographics, severity levels, acquisition parameters
- **Approach**: Transfer learning; fine-tune on new dataset with small labeled set

#### 8. Language model integration
- **Task**: Improve word-level accuracy via phoneme‚Üíword decoding
- **Current**: CMU lexicon lookup (no context modeling)
- **Enhancement**: Add n-gram LM or neural LM for word-level constraints

#### 9. Clinician-facing diagnostic tool
- **Task**: Create interface for speech-language pathologists (SLPs)
- **Features**:
  - Real-time phoneme recognition with confidence
  - Per-speaker error patterns (substitutions, deletions, insertions)
  - Severity estimation
  - Comparison to baseline cohorts
- **Output**: Web or desktop dashboard

---

## Integration with Clinical Workflows

### Target Users
- **Speech-language pathologists (SLPs)**: Need explainable, per-phoneme feedback
- **Researchers**: Need reproducible baselines and ablation studies
- **Patients**: Need real-time feedback during therapy sessions

### Deployment Requirements
- **Inference latency**: <100ms per utterance
- **Batch inference**: Support patients doing homework exercises (offline processing)
- **Privacy**: Local processing (no cloud upload) for sensitive health data
- **Explainability**: Rule-based constraint transparency (SLPs trust interpretable models)

### Success Criteria
1. **Clinical validation**: Compare DysarthriaNSR vs. human SLP transcription on 50+ utterances
2. **User feedback**: SLPs improve therapy outcomes using system feedback (pilot study)
3. **Publication**: Peer-reviewed paper showing improvements over standard ASR

---

## Common Development Patterns & Code Reusability

### Adding a new metric
1. Implement in `evaluate.py` (as standalone function)
2. Test with sample predictions
3. Integrate into `evaluate_model()` in `train.py`
4. Log to MLflow

### Changing hyperparameters
1. Edit defaults in `src/utils/config.py` (not in train.py)
2. Use `Config()` to load; modify only if dynamic
3. Parameters auto-logged to MLflow

### Adding a new loss term
1. Implement in `DysarthriaASRLightning.compute_loss()`
2. Add weight to `TrainingConfig` (e.g., `lambda_new_loss`)
3. Update `train_or_val_step` to compute loss
4. Monitor via MLflow logger

### Debugging a training issue
1. Check board: `tensorboard --logdir mlruns`
2. Inspect latest checkpoint: `torch.load(f"checkpoints/{run_name}/last.ckpt")`
3. Use `evaluate.py` utilities to analyze predictions
4. Check raw data: `pd.read_csv("data/processed/torgo_neuro_symbolic_manifest.csv").head()`

---

## Bibliography & Key References

- **HuBERT**: Hsu et al., "HuBERT: Self-supervised Speech Representation Learning by Masked Prediction of Hidden Units" (2021)
- **TORGO**: Rudzicz et al., "The TORGO Database of Acoustic and Articulatory Speech from Dysarthric Speakers" (2012)
- **Dysarthric phonology**: Kent & Rosen, "Motor Control Perspectives on Motor Speech Disorders" (2004)
- **CTC Loss**: Graves et al., "Towards End-to-End Speech Recognition with RNNs" (2014)
- **Neuro-symbolic integration**: Mao et al., "The Neuro-Symbolic Concept Learner" (2019)

---

## Development Checklist for Contributors

### Before submitting a PR
- [ ] Code follows Black style (88 char lines)
- [ ] All functions have docstrings with Args/Returns/Raises
- [ ] Type hints on function signatures (avoid `Any`)
- [ ] New metrics tested on sample predictions
- [ ] MLflow logging includes appropriate parameters
- [ ] No hardcoded paths (use `ProjectPaths`)
- [ ] Phoneme strings normalized via `normalize_phoneme()`
- [ ] Test on RTX 4060 (8GB VRAM limit)

### Testing
```bash
# Unit test config loading
python -c "from src.utils.config import Config; c = Config()"

# Quick dataloader test
python -c "from src.data.dataloader import TorgoNeuroSymbolicDataset; ds = TorgoNeuroSymbolicDataset('data/processed/torgo_neuro_symbolic_manifest.csv'); print(ds[0].keys())"

# Training smoke test (1 epoch, 10 batches)
python train.py --run-name smoke_test --max_epochs 1 --limit_train_batches 10

# Evaluation test
python evaluate.py
```

**N-gram LM Training:**
- Train on general text corpus (LibriSpeech transcripts)
- Fine-tune on TORGO transcripts (limited data ‚Üí importance of general LM)
- Model: 3-gram with Kneser-Ney smoothing

**Rescoring with Neural LM (Optional):**
- Generate N-best hypotheses (N=10) from beam search
- Rescore with LSTM or Transformer LM
- Select best hypothesis based on combined score

---

## 6. Explainability Module

### 6.1 Phoneme Attribution Analysis

**Goal:** Identify which phonemes caused ASR errors

**Method 1: Alignment-Based Attribution**
```python
def phoneme_error_attribution(predicted_phonemes, ground_truth_phonemes):
    # Step 1: Align sequences using edit distance
    alignment = levenshtein_alignment(predicted_phonemes, ground_truth_phonemes)
    
    # Step 2: Categorize errors
    errors = {
        'substitutions': [],  # [(predicted, expected, position)]
        'deletions': [],      # [(expected, position)]
        'insertions': []      # [(predicted, position)]
    }
    
    for op, ph_pred, ph_true, pos in alignment:
        if op == 'substitute':
            errors['substitutions'].append((ph_pred, ph_true, pos))
        elif op == 'delete':
            errors['deletions'].append((ph_true, pos))
        elif op == 'insert':
            errors['insertions'].append((ph_pred, pos))
    
    # Step 3: Map to articulatory features
    feature_errors = analyze_feature_patterns(errors)
    return errors, feature_errors
```

**Method 2: Attention-Based Attribution**
```python
# Extract attention weights from transformer layers
attention_weights = model.get_attention_weights()  # [layers, heads, frames, frames]

# Aggregate attention to phoneme boundaries
phoneme_attention = aggregate_attention_to_phonemes(
    attention_weights, 
    phoneme_boundaries
)

# Identify high-attention phonemes in error regions
error_phoneme_attention = phoneme_attention[error_frames]
```

### 6.2 Symbolic Rule Activation Tracking

**Rule Logging System:**

```python
class SymbolicRuleTracker:
    def __init__(self):
        self.activations = []
    
    def log_rule_activation(self, rule_id, input_context, output_correction, confidence):
        self.activations.append({
            'rule_id': rule_id,
            'input': input_context,
            'output': output_correction,
            'confidence': confidence,
            'timestamp': current_frame()
        })
    
    def generate_explanation(self):
        explanation = {
            'total_rules_fired': len(self.activations),
            'high_confidence_corrections': [
                a for a in self.activations if a['confidence'] > 0.7
            ],
            'rule_frequency': Counter([a['rule_id'] for a in self.activations])
        }
        return explanation
```

### 6.3 Feature-Level Error Analysis

**Articulatory Feature Confusion Matrix:**

```python
# Build confusion matrix for each feature dimension
feature_dims = ['manner', 'place', 'voice']

for feature in feature_dims:
    confusion[feature] = defaultdict(lambda: defaultdict(int))
    
    for (pred_ph, true_ph, pos) in substitution_errors:
        pred_val = phoneme_features[pred_ph][feature]
        true_val = phoneme_features[true_ph][feature]
        confusion[feature][true_val][pred_val] += 1

# Visualization: Heatmap for each feature dimension
plot_confusion_heatmap(confusion['manner'], title='Manner of Articulation Errors')
plot_confusion_heatmap(confusion['place'], title='Place of Articulation Errors')
plot_confusion_heatmap(confusion['voice'], title='Voicing Errors')
```

### 6.4 Explainable Output Format

**Structured Explanation JSON:**

```json
{
  "utterance_id": "M01_session1_sentence5",
  "ground_truth": "THE QUICK BROWN FOX",
  "prediction": "THE QICK BOWN FOX",
  "wer": 0.25,
  "phoneme_analysis": {
    "errors": [
      {
        "type": "deletion",
        "position": 5,
        "expected_phoneme": "W",
        "context": "K_IH_[W]_IH_K",
        "articulatory_features": {
          "manner": "approximant",
          "place": "labio-velar",
          "voice": "voiced"
        },
        "probable_cause": "Cluster simplification (KW‚ÜíK)",
        "symbolic_rule_activated": "rule_42_cluster_reduction"
      },
      {
        "type": "substitution",
        "position": 12,
        "predicted_phoneme": "W",
        "expected_phoneme": "R",
        "feature_differences": {
          "place": "labio-velar vs. alveolar"
        },
        "probable_cause": "Liquid gliding (R‚ÜíW)",
        "neural_confidence": 0.62,
        "symbolic_correction_applied": true
      }
    ]
  },
  "symbolic_rules_summary": {
    "total_fired": 3,
    "rule_details": [
      {"rule_id": 42, "name": "cluster_reduction", "confidence": 0.85},
      {"rule_id": 17, "name": "liquid_gliding", "confidence": 0.70}
    ]
  },
  "attention_visualization": "base64_encoded_heatmap"
}
```

---

## 7. Training Pipeline

### 7.1 Training Data Split

```
TORGO Dataset Split:
‚îú‚îÄ‚îÄ Train: 70% (stratified by speaker severity)
‚îú‚îÄ‚îÄ Validation: 15% (different speakers from train)
‚îî‚îÄ‚îÄ Test: 15% (held-out speakers, unseen during training)

Cross-Validation Strategy:
- Leave-One-Speaker-Out (LOSO) for generalization testing
- Ensures model doesn't overfit to specific dysarthric patterns
```

### 7.2 Training Hyperparameters

```python
training_config = {
    # Optimization
    'optimizer': 'AdamW',
    'learning_rate': 5e-5,
    'weight_decay': 0.01,
    'lr_scheduler': 'OneCycleLR',
    'warmup_ratio': 0.1,  # 10% of total steps
    
    # Training dynamics (VRAM-optimized for RTX 4060 8GB)
    'batch_size': 1,  # Further reduced from 2
    'gradient_accumulation': 8,  # Effective batch size = 8
    'max_epochs': 5,  # For experimentation; increase for convergence
    'early_stopping_patience': 10,
    
    # Regularization
    'dropout': 0.1,
    'layer_dropout': 0.05,  # Drop entire transformer layers
    'label_smoothing': 0.1,
    
    # Loss weights (fixed, not dynamic)
    'lambda_ctc': 0.7,  # Primary: phoneme alignment
    'lambda_ce': 0.3,   # Auxiliary: frame-level classification
    
    # Precision & Memory
    'precision': '16-mixed',  # Mixed precision for GPU efficiency
    'gradient_checkpointing': True,  # Trade compute for memory
    'max_audio_length': 8.0,  # Truncate audio to 8 seconds
}
```

**Memory Optimizations in Detail**:
- **Gradient Checkpointing** ([src/models/model.py](src/models/model.py#L325)): Recompute activations during backward pass instead of storing them. Reduces memory by ~40% at cost of ~20% slower training.
- **Audio Truncation** ([src/data/dataloader.py](src/data/dataloader.py#L168)): Limit input length to 8 seconds (~128k samples at 16kHz). Prevents pathological long-sequence memory spikes.
- **Frozen Encoder Layers**: Freeze first 8 of 12 HuBERT layers ([src/models/model.py](src/models/model.py#L352)). Low-level acoustic features don't need fine-tuning for dysarthria.
- **Mixed Precision (fp16)**: Use 16-bit floats for activations/weights, 32-bit for loss computation. PyTorch Lightning handles this automatically.
- **CUDA Fragmentation Mitigation** ([train.py](train.py#L551)): Set `PYTORCH_ALLOC_CONF=expandable_segments:True` to allow dynamic memory pool resizing.

### 7.3 Training Loop

```python
for epoch in range(max_epochs):
    for batch in train_loader:
        # Forward pass
        audio, phoneme_labels, word_labels = batch
        
        # 1. HuBERT encoding
        hidden_states = hubert_model(audio)
        
        # 2. Phoneme prediction
        phoneme_logits = phoneme_classifier(hidden_states)
        
        # 3. Apply symbolic constraints
        phoneme_probs = symbolic_layer(phoneme_logits, speaker_severity)
        
        # 4. Compute multi-task loss
        loss_ctc = ctc_loss(phoneme_probs, phoneme_labels)
        loss_ce = cross_entropy(phoneme_logits, phoneme_labels)
        loss_focal = focal_loss(phoneme_logits, phoneme_labels)
        loss_symbolic = constraint_violation_loss(phoneme_probs)
        
        total_loss = (lambda_ctc * loss_ctc + 
                      lambda_ce * loss_ce + 
                      lambda_focal * loss_focal +
                      lambda_symbolic * loss_symbolic)
        
        # 5. Backward pass
        total_loss.backward()
        
        # 6. Gradient clipping
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        
        # 7. Optimizer step
        optimizer.step()
        optimizer.zero_grad()
    
    # Validation
    val_metrics = evaluate(model, val_loader)
    
    # Early stopping
    if val_metrics['wer'] < best_wer:
        best_wer = val_metrics['wer']
        save_checkpoint(model)
        patience_counter = 0
    else:
        patience_counter += 1
        if patience_counter >= early_stopping_patience:
            break
```

---

## 8. Evaluation Metrics

### 8.1 ASR Performance Metrics

**Word Error Rate (WER):**
```python
WER = (S + D + I) / N

Where:
- S = number of substitutions
- D = number of deletions
- I = number of insertions
- N = total words in reference
```

**Phoneme Error Rate (PER):**
```python
PER = (S_ph + D_ph + I_ph) / N_ph
```

**Character Error Rate (CER):**
- Useful for partial credit in dysarthric speech

### 8.2 Dysarthria-Specific Metrics

**Severity-Stratified WER:**
```python
wer_by_severity = {
    'mild': compute_wer(predictions[severity=='mild']),
    'moderate': compute_wer(predictions[severity=='moderate']),
    'severe': compute_wer(predictions[severity=='severe'])
}
```

**Intelligibility Correlation:**
- Pearson correlation between predicted WER and clinical intelligibility scores

### 8.3 Explainability Metrics

**Rule Precision:**
```python
rule_precision = (correct_rule_applications) / (total_rule_applications)
```

**Attribution Consistency:**
- Inter-rater agreement between model explanations and expert phonetic analysis

### 8.4 Stratified Evaluation and Statistical Testing

**Length-Stratified PER:**
- Buckets: 0-5, 6-10, 11-20, 21+ phonemes
- Report mean PER with bootstrap confidence intervals per bucket

**Speaker-Stratified PER:**
- Per-speaker PER with mean and std
- Compare dysarthric vs control with statistical testing

**Statistical Tests:**
- Paired tests for model comparisons (paired t-test or Wilcoxon)
- Group tests for dysarthric vs control (Welch t-test or Mann-Whitney U)
- Correct for multiple comparisons (Holm or Bonferroni)

---

## 9. System Integration & Deployment

### 9.1 Inference Pipeline

```python
def inference_pipeline(audio_file, speaker_severity=None):
    # 1. Preprocessing
    audio = load_audio(audio_file, target_sr=16000)
    audio = normalize(audio)
    
    # 2. HuBERT encoding
    with torch.no_grad():
        hidden_states = hubert_model(audio)
    
    # 3. Phoneme prediction
    phoneme_logits = phoneme_classifier(hidden_states)
    phoneme_probs = symbolic_layer(phoneme_logits, speaker_severity)
    
    # 4. CTC decoding ‚Üí phoneme sequence
    phoneme_sequence = ctc_decode(phoneme_probs)
    
    # 5. Lexicon lookup + beam search
    word_sequence = beam_search_decode(phoneme_sequence, lexicon, lm)
    
    # 6. Generate explanations
    explanations = explainability_module.analyze(
        phoneme_sequence, 
        ground_truth=None,  # If available
        attention_weights=hidden_states
    )
    
    return {
        'transcription': ' '.join(word_sequence),
        'phonemes': phoneme_sequence,
        'explanations': explanations
    }
```

### 9.2 Model Serving Architecture

```
Client Request (Audio)
    ‚Üì
API Gateway (FastAPI)
    ‚Üì

### 9.3 ONNX Export Checklist

1. Export HuBERT encoder + classifier + symbolic layer with dynamic axes.
2. Validate ONNX parity on a small batch (logits + PER spot check).
3. Run ONNX Runtime CPU inference for a short sample.
4. Optional: keep CTC decoding outside ONNX to avoid dynamic control flow.
5. Package model and inference config for deployment.
Preprocessing Service
    ‚Üì
GPU Inference Service (HuBERT + Phoneme Model)
    ‚Üì
Decoding Service (CPU-based beam search)
    ‚Üì
Explainability Service
    ‚Üì
Response (Transcription + Explanations)
```

---

## Implementation Checklist

### ‚úÖ Data & Preprocessing
- [x] TORGO dataset download via HuggingFace ([download.py](download.py))
- [x] Manifest generation with phoneme extraction ([manifest.py](manifest.py))
- [x] Articulatory feature mapping (stops, fricatives, nasals, liquids, glides, vowels)
- [x] RMS energy calculation for signal quality assessment
- [x] Speaker-prefixed sample matching

### ‚úÖ Neural Component
- [x] HuBERT encoder initialization (facebook/hubert-base-ls960)
- [x] Selective layer freezing (layers 0-5 frozen, 6-11 fine-tuned)
- [x] Projection layer (768 ‚Üí 512-dim)
- [x] Phoneme classifier with dropout

### ‚úÖ Symbolic Component
- [x] ArticulatoryFeatureEncoder (manner, place, voicing features)
- [x] SymbolicConstraintMatrix (euclidean distance-based similarity)
- [x] ConstraintAggregation (weighted combination of neural + symbolic logits)
- [x] Dysarthria-aware weighting (severity-dependent interpolation)

### ‚úÖ Training
- [x] PyTorch Lightning trainer setup ([train.py](train.py))
- [x] Multi-task learning (CTC + Focal + KL constraint losses)
- [x] Gradient accumulation & VRAM optimization
- [x] Learning rate scheduling (cosine annealing + warmup)
- [x] Callbacks (EarlyStopping, ModelCheckpoint, LearningRateMonitor)
- [x] MLflow experiment tracking with safe parameter flattening

### ‚úÖ Evaluation
- [x] PER (Phoneme Error Rate) computation ([evaluate.py](evaluate.py))
- [x] WER (Word Error Rate) with jiwer
- [x] Phoneme alignment & confusion matrices
- [x] Per-speaker and per-articulatory-class error analysis
- [x] Visualization utilities

### ‚úÖ Data Management
- [x] PyTorch Dataset & DataLoader ([dataloader.py](dataloader.py))
- [x] CTC-compatible batching (attention masks, label padding)
- [x] Peak normalization for dysarthric speech variability
- [x] Collator for flexible sequence handling

### ‚úÖ Configuration
- [x] Dataclass-based config ([config.py](config.py))
- [x] ModelConfig, TrainingConfig, SymbolicConfig, DataConfig
- [x] Safe YAML loading & parameter override

---

## üìã Not Yet Implemented

### Explainability & Analysis
- [ ] Interactive dashboard (phoneme confusion heatmaps, rule activation maps)
- [ ] Per-speaker error attribution with clinical interpretability
- [ ] Rule activation tracking for symbolic constraint layer
- [ ] Attention visualization for HuBERT layer analysis

### Model Deployment & Checkpoints
- [ ] Pretrained model checkpoints (dysarthric/control baselines)
- [ ] ONNX export for inference optimization
- [ ] TorchScript export for production deployment
- [ ] Streaming inference API (FastAPI)

### Advanced Features
- [ ] Automatic dysarthric substitution rule discovery (from confusion matrices)
- [ ] Speaker adaptation fine-tuning
- [ ] Confidence scoring & uncertainty estimation
- [ ] Real-time feedback system for speech therapy

### Research Extensions
- [ ] Ablation studies (neural vs. symbolic contribution quantification)
- [ ] Cross-dataset evaluation (on non-TORGO dysarthric corpora)
- [ ] Comparison with baseline ASR systems
- [ ] Analysis of constraint matrix learned weights

---

## Key Design Decisions & Rationale

### Why HuBERT over Wav2Vec2?
- HuBERT uses discrete clustering targets (more phoneme-aligned)
- Better performance on dysarthric speech in preliminary experiments
- Smaller parameter footprint for VRAM constraints

### Why Symbolic Constraints?
- Dysarthric substitutions follow articulatory patterns (preserve place/manner)
- Symbolic layer provides interpretability (which rules fired?)
- Modular design allows separate ablation of neural vs. symbolic components

### Why Multi-Task Learning?
- Dysarthria classification as auxiliary task improves speaker-level discrimination
- Focal loss handles class imbalance (fewer dysarthric speakers)
- KL constraint loss provides soft regularization (not hard constraints)

### Why Gradient Accumulation?
- RTX 4060 (8GB VRAM) cannot fit batch_size > 2
- Accumulation steps=12 ‚Üí effective batch=24 (better gradient estimates)
- Maintains training stability without reducing effective batch size

### Why Speaker-Independent Splits?
- TORGO has only ~15 speakers; speaker-dependent splits would leak test information
- Generalization to unseen dysarthric speakers is the research goal
- Per-speaker evaluation metrics computed separately

---

## Integration Points & Cross-Component Communication

### Data ‚Üí Model Flow
```
manifest.csv (speaker, phonemes, articulatory_classes)
    ‚Üì
TorgoNeuroSymbolicDataset (streams from HF + manifest)
    ‚Üì
NeuroSymbolicCollator (batches with attention masks, label padding=-100)
    ‚Üì
DataLoader (shuffle, variable-length batching)
    ‚Üì
NeuroSymbolicASR (forward pass)
```

### Model Inference Flow
```
Audio Waveform (16kHz)
    ‚Üì
HuBERT Feature Extraction (768-dim, 50Hz frame rate)
    ‚Üì
Projection (768 ‚Üí 512)
    ‚Üì
Phoneme Classifier ‚Üí Neural Logits
    ‚Üì
SymbolicConstraintMatrix (precomputed from phoneme features)
    ‚Üì
ConstraintAggregation (Œ± * neural + (1-Œ±) * (C @ neural))
    ‚Üì
Constrained Logits
    ‚Üì
CTC Decoder (greedy or beam search)
    ‚Üì
Phoneme Sequence ‚Üí Transcript
```

### Training Loop
```
Batch from DataLoader
    ‚Üì
Forward pass ‚Üí neural logits + constrained logits
    ‚Üì
CTC Loss (alignment) + Focal Loss (dysarthria) + KL Loss (constraint)
    ‚Üì
Backward pass (gradient accumulation every N steps)
    ‚Üì
Learning rate scheduler update
    ‚Üì
Validation: PER computation on val set
    ‚Üì
Early stopping (metric: val_per, patience=5)
```

---

## Common Debugging Patterns

**Issue**: Audio appears silent in batch
- **Cause**: Peak normalization not applied during loading
- **Solution**: Verify `_load_audio()` normalizes by `max(abs(waveform))`

**Issue**: CTC loss NaN or infinite
- **Cause**: Input lengths > output lengths (feature extraction stride issue)
- **Solution**: Check HuBERT feature rate: ~50Hz for 16kHz audio (stride=320)

**Issue**: Model not learning (loss plateaus)
- **Cause**: Symbolic constraint too strong (Œ± too low), masking neural learning
- **Solution**: Increase Œ± (start 0.8, decrease to 0.5 for dysarthric cohorts)

**Issue**: Validation PER very high
- **Cause**: Speaker-prefixed sample IDs not matching HF dataset keys
- **Solution**: Verify manifest `sample_id` format: `f"{speaker}_{filename}"`
